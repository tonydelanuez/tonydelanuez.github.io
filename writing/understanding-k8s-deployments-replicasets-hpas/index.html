<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How Kubernetes Deployments, ReplicaSets, and HPAs coordinate | Tony De La Nuez</title>
<meta name=keywords content="tech,kubernetes,hpa"><meta name=description content="Let's walk through a real-world scenario where a Kubernetes Deployment rollout and HPA clashed yet everything was technically working. Here's how these important control loops interact under pressure and how to debug autoscaling failures that don’t show up in staging."><meta name=author content="Tony"><link rel=canonical href=https://tdoot.com/writing/understanding-k8s-deployments-replicasets-hpas/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.b01d27ec677119335be4209671dcc7e81806d41e74f1dfc7558e6f2ff92cc75d.css integrity="sha256-sB0n7GdxGTNb5CCWcdzH6BgG1B508d/HVY5vL/ksx10=" rel="preload stylesheet" as=style><link rel=icon href=https://tdoot.com/images/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tdoot.com/images/icons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tdoot.com/images/icons/favicon-32x32.png><link rel=apple-touch-icon href=https://tdoot.com/images/icons/apple-touch-icon.png><link rel=mask-icon href=https://tdoot.com/images/icons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Alex+Brush&display=swap" rel=stylesheet><link rel=alternate hreflang=en href=https://tdoot.com/writing/understanding-k8s-deployments-replicasets-hpas/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y62CJP0177"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y62CJP0177")}</script><meta property="og:url" content="https://tdoot.com/writing/understanding-k8s-deployments-replicasets-hpas/"><meta property="og:site_name" content="Tony De La Nuez"><meta property="og:title" content="How Kubernetes Deployments, ReplicaSets, and HPAs coordinate"><meta property="og:description" content="Let's walk through a real-world scenario where a Kubernetes Deployment rollout and HPA clashed yet everything was technically working. Here's how these important control loops interact under pressure and how to debug autoscaling failures that don’t show up in staging."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="writing"><meta property="article:published_time" content="2025-05-03T12:01:14-05:00"><meta property="article:modified_time" content="2025-05-03T12:01:14-05:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Hpa"><meta property="og:image" content="https://tdoot.com/images/hpa.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tdoot.com/images/hpa.png"><meta name=twitter:title content="How Kubernetes Deployments, ReplicaSets, and HPAs coordinate"><meta name=twitter:description content="Let's walk through a real-world scenario where a Kubernetes Deployment rollout and HPA clashed yet everything was technically working. Here's how these important control loops interact under pressure and how to debug autoscaling failures that don’t show up in staging."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://tdoot.com/writing/"},{"@type":"ListItem","position":2,"name":"How Kubernetes Deployments, ReplicaSets, and HPAs coordinate","item":"https://tdoot.com/writing/understanding-k8s-deployments-replicasets-hpas/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How Kubernetes Deployments, ReplicaSets, and HPAs coordinate","name":"How Kubernetes Deployments, ReplicaSets, and HPAs coordinate","description":"Let's walk through a real-world scenario where a Kubernetes Deployment rollout and HPA clashed yet everything was technically working. Here's how these important control loops interact under pressure and how to debug autoscaling failures that don’t show up in staging.","keywords":["tech","kubernetes","hpa"],"articleBody":"This is the third post in my Kubernetes Controllers series. In the first post, we explored what controllers are and how they maintain your desired state through continuous reconciliation. In the second post I walk through how to build stable controllers from scratch. Now we’ll examine how three critical controllers interact to manage your applications.\nWe were rolling out a new version of an application. 100 pods, all running about 25% hot on CPU.\nThe HPA didn’t scale up. The rollout was throttled. Metrics were delayed. Traffic took a hit.\nThe frustrating part? Everything was working exactly as designed. The HPA wasn’t scaling because it was configured not to.\nKubernetes is a system of nested control loops. HPAs scale Deployments. Deployments manage ReplicaSets. ReplicaSets spin up Pods. Each loop has its own knobs. But what matters is how those loops interact, especially in moments like this.\nTuning your cluster isn’t about memorizing flags. It’s about understanding where the edges are and what happens when you push into them.\nCore control loops As I mentioned in the first post of the series, Kubernetes is built on reconciliation. Deployments, ReplicaSets, and HPAs are each their own control loop:\nThe HPA loop observes metrics and adjusts the replica count on a Deployment\nThe Deployment loop reconciles the desired replica count by adjusting ReplicaSets\nThe ReplicaSet loop reconciles its pod count by creating or deleting Pods\nEach loop observes state, compares it with the desired target, and takes corrective action. If you want to understand what’s actually happening with your application deployments, it’s important to learn how to spot what actions are being taken and understand why.\nInspecting status fields These control loops report their current state in .status fields. These are the first things you should check when something doesn’t look right.\nDeployment status kubectl get deployment demo -o yaml | yq .status observedGeneration: The latest generation the controller has processed\nreplicas, updatedReplicas, availableReplicas: rollout progress\nconditions[]:\nProgressing: whether a rollout is ongoing\nAvailable: whether minimum availability has been met\nIf a rollout seems stuck, check if Progressing=True with a reason like ReplicaSetUpdated. If so, you’re likely waiting on pods to become Ready. Once the pods fully come up and report Ready, the Deployment will transition its Available condition to Status=True, Reason=# MinimumReplicasAvailable\nReplicaSet status kubectl get rs -o yaml | yq '.items[] | {name: .metadata.name, status: .status}' Look at replicas, readyReplicas, and availableReplicas. Those show how far along each ReplicaSet is. If you see a conditions[].type=ReplicaFailure, it might be a scheduling or crash loop issue. Debug with kubectl describe pod and check out the Events field.\nHPA status kubectl describe hpa Check the current vs. target CPU utilization. Look at desiredReplicas vs. currentReplicas. And scan the Conditions[] field for clues like ScalingLimited, AbleToScale, or ScalingActive.\nIf you want to dig into metrics:\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq How ReplicaSets relate to Deployments Deployments don’t create Pods directly. They create ReplicaSets. If the pod template changes (image, labels, env), a new ReplicaSet is created. The Deployment scales the new one up and the old one down. For example, take this sample deployment that manages an nginx app:\nkubectl apply -f - \u003c","wordCount":"2301","inLanguage":"en","image":"https://tdoot.com/images/hpa.png","datePublished":"2025-05-03T12:01:14-05:00","dateModified":"2025-05-03T12:01:14-05:00","author":{"@type":"Person","name":"Tony"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tdoot.com/writing/understanding-k8s-deployments-replicasets-hpas/"},"publisher":{"@type":"Organization","name":"Tony De La Nuez","logo":{"@type":"ImageObject","url":"https://tdoot.com/images/icons/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://tdoot.com/ accesskey=h title="tdoot (Alt + H)"><img src=https://tdoot.com/images/icons/apple-touch-icon.png alt aria-label=logo height=15>tdoot</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://tdoot.com/categories/writing title=writing><span>writing</span></a></li><li><a href=https://tdoot.com/photos title=photos><span>photos</span></a></li><li><a href=https://tdoot.com/reading title=reading><span>reading</span></a></li><li><a href=https://tdoot.com/life title=life><span>life</span></a></li><li><a href=https://tdoot.com/tags title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tdoot.com/>Home</a>&nbsp;»&nbsp;<a href=https://tdoot.com/writing/></a></div><h1 class="post-title entry-hint-parent">How Kubernetes Deployments, ReplicaSets, and HPAs coordinate</h1><div class=post-description>Let's walk through a real-world scenario where a Kubernetes Deployment rollout and HPA clashed yet everything was technically working. Here's how these important control loops interact under pressure and how to debug autoscaling failures that don’t show up in staging.</div><div class=post-meta><span title='2025-05-03 12:01:14 -0500 -0500'>May 3, 2025</span>&nbsp;·&nbsp;Tony</div></header><div class=post-content><p><em>This is the third post in my Kubernetes Controllers series. In the <a href=/writing/kubernetes-controllers-explained>first post</a>, we explored what controllers are and how they maintain your desired state through continuous reconciliation. In the <a href=/writing/building-solid-kubernetes-controllers>second post</a> I walk through how to build stable controllers from scratch. Now we&rsquo;ll examine how three critical controllers interact to manage your applications.</em></p><hr><p>We were rolling out a new version of an application. 100 pods, all running about 25% hot on CPU.</p><p>The HPA didn’t scale up. The rollout was throttled. Metrics were delayed. Traffic took a hit.</p><p>The frustrating part? Everything was working exactly as designed. The HPA wasn’t scaling because it was configured not to.</p><p>Kubernetes is a system of nested control loops. HPAs scale Deployments. Deployments manage ReplicaSets. ReplicaSets spin up Pods. Each loop has its own knobs. But what matters is how those loops interact, especially in moments like this.</p><p>Tuning your cluster isn&rsquo;t about memorizing flags. It’s about understanding where the edges are and what happens when you push into them.</p><hr><h2 id=core-control-loops>Core control loops<a hidden class=anchor aria-hidden=true href=#core-control-loops>#</a></h2><p>As I mentioned in the first post of the series, Kubernetes is built on reconciliation. Deployments, ReplicaSets, and HPAs are each their own control loop:</p><ul><li><p>The HPA loop observes metrics and adjusts the replica count on a Deployment</p></li><li><p>The Deployment loop reconciles the desired replica count by adjusting ReplicaSets</p></li><li><p>The ReplicaSet loop reconciles its pod count by creating or deleting Pods</p></li></ul><p><img alt="HPAs, ReplicaSets, Deployments" loading=lazy src=/images/pictures/hpas.png></p><p>Each loop observes state, compares it with the desired target, and takes corrective action. If you want to understand what&rsquo;s actually happening with your application deployments, it&rsquo;s important to learn how to spot what actions are being taken and understand why.</p><hr><h2 id=inspecting-status-fields>Inspecting status fields<a hidden class=anchor aria-hidden=true href=#inspecting-status-fields>#</a></h2><p>These control loops report their current state in <code>.status</code> fields. These are the first things you should check when something doesn’t look right.</p><h3 id=deployment-status>Deployment status<a hidden class=anchor aria-hidden=true href=#deployment-status>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get deployment demo -o yaml <span class=p>|</span> yq .status
</span></span></code></pre></div><ul><li><p><code>observedGeneration</code>: The latest generation the controller has processed</p></li><li><p><code>replicas</code>, <code>updatedReplicas</code>, <code>availableReplicas</code>: rollout progress</p></li><li><p><code>conditions[]</code>:</p><ul><li><p><code>Progressing</code>: whether a rollout is ongoing</p></li><li><p><code>Available</code>: whether minimum availability has been met</p></li></ul></li></ul><p>If a rollout seems stuck, check if <code>Progressing=True</code> with a <code>reason</code> like <code>ReplicaSetUpdated</code>. If so, you’re likely waiting on pods to become Ready. Once the pods fully come up and report <code>Ready</code>, the Deployment will transition its Available condition to <code>Status=True</code>, <code>Reason=# MinimumReplicasAvailable</code></p><h3 id=replicaset-status>ReplicaSet status<a hidden class=anchor aria-hidden=true href=#replicaset-status>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get rs -o yaml <span class=p>|</span> yq <span class=s1>&#39;.items[] | {name: .metadata.name, status: .status}&#39;</span>
</span></span></code></pre></div><p>Look at <code>replicas</code>, <code>readyReplicas</code>, and <code>availableReplicas</code>. Those show how far along each ReplicaSet is. If you see a <code>conditions[].type=ReplicaFailure</code>, it might be a scheduling or crash loop issue. Debug with <code>kubectl describe pod &lt;foo></code> and check out the Events field.</p><h3 id=hpa-status>HPA status<a hidden class=anchor aria-hidden=true href=#hpa-status>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl describe hpa
</span></span></code></pre></div><p>Check the current vs. target CPU utilization. Look at <code>desiredReplicas</code> vs. <code>currentReplicas</code>. And scan the <code>Conditions[]</code> field for clues like <code>ScalingLimited</code>, <code>AbleToScale</code>, or <code>ScalingActive</code>.</p><p>If you want to dig into metrics:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get --raw <span class=s2>&#34;/apis/custom.metrics.k8s.io/v1beta1&#34;</span> <span class=p>|</span> jq
</span></span></code></pre></div><hr><h2 id=how-replicasets-relate-to-deployments>How ReplicaSets relate to Deployments<a hidden class=anchor aria-hidden=true href=#how-replicasets-relate-to-deployments>#</a></h2><p>Deployments don’t create Pods directly. They create ReplicaSets. If the pod template changes (image, labels, env), a new ReplicaSet is created. The Deployment scales the new one up and the old one down.
For example, take this sample deployment that manages an nginx app:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f - <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: apps/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Deployment
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: demo-deployment
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  replicas: 3
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    matchLabels:
</span></span></span><span class=line><span class=cl><span class=s>      app: demo
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    metadata:
</span></span></span><span class=line><span class=cl><span class=s>      labels:
</span></span></span><span class=line><span class=cl><span class=s>        app: demo
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      containers:
</span></span></span><span class=line><span class=cl><span class=s>      - name: demo
</span></span></span><span class=line><span class=cl><span class=s>        image: nginx:1.14.2
</span></span></span><span class=line><span class=cl><span class=s>        ports:
</span></span></span><span class=line><span class=cl><span class=s>        - containerPort: 80
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><p>Trigger a rollout to see the rollingUpdate in action:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl <span class=nb>set</span> image deployment/demo-deployment <span class=nv>demo</span><span class=o>=</span>nginx:1.16.1
</span></span></code></pre></div><p>Watch it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get rs
</span></span><span class=line><span class=cl>kubectl get events --sort-by<span class=o>=</span>.metadata.creationTimestamp
</span></span></code></pre></div><p>You&rsquo;ll see that the Deployment creates a new ReplicaSet and starts scaling up the new ReplicaSet while scaling down the old.</p><hr><h2 id=why-not-manage-replicasets-directly>Why not manage ReplicaSets directly?<a hidden class=anchor aria-hidden=true href=#why-not-manage-replicasets-directly>#</a></h2><p>You could. But you’d be reimplementing rollbacks, surge logic, status tracking, and GC. Deployments handle that for you. For most workloads, Deployments give you the functionality you need to safely upgrade, automatically recycle unhealthy pods, and allow you to scale with changes in load when you attach an HPA to them.</p><p>That said, some tools (like <a href=https://argoproj.github.io/rollouts/>Argo Rollouts</a>) bypass Deployments and manage ReplicaSets directly. That gives them fine-grained control over metrics, gates, and promotion strategies. If you need canary, blue/green, or experiment-based rollout logic, this is usually why.</p><hr><h2 id=tuning-rollout-behavior>Tuning rollout behavior<a hidden class=anchor aria-hidden=true href=#tuning-rollout-behavior>#</a></h2><p>If your application is experiencing some issues during deploys, take a look at tuning your rollout config.</p><p>By default, a Deployment will roll pods out a few at a time in attempt to maintain availability. But depending on your app, infra, and traffic patterns, the defaults could either not be aggressive enough or might be too aggressive.</p><p>Here’s what that looks like in config (you can see this on your Deployment object):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>strategy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>RollingUpdate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rollingUpdate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>maxSurge</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>maxUnavailable</span><span class=p>:</span><span class=w> </span><span class=m>0</span><span class=w>
</span></span></span></code></pre></div><p>You can tune these to shape how rollouts behave:</p><table><thead><tr><th>Goal</th><th>maxSurge</th><th>maxUnavailable</th><th>Behavior</th></tr></thead><tbody><tr><td>Faster rollouts</td><td>High</td><td>High</td><td>More pods created/terminated in parallel</td></tr><tr><td>High stability</td><td>Low</td><td>0</td><td>Minimize disruption, one pod at a time</td></tr><tr><td>Resource constrained</td><td>0</td><td>1</td><td>No surge pods, slower rollout</td></tr></tbody></table><p>If you’ve got lots of traffic and startup takes a while, more <code>maxSurge</code> gives you headroom. If you’re tight on memory or have flaky readiness probes, you might want to slow things down.</p><p>There’s no perfect setting, just tradeoffs you can make based on profiling of your app. But it’s worth knowing that this knob exists before your next incident forces you to care.</p><hr><h2 id=back-to-the-opening-issue-hpa-doesnt-scale-try-reproducing-it>Back to the opening issue: HPA doesn’t scale? Try reproducing it<a hidden class=anchor aria-hidden=true href=#back-to-the-opening-issue-hpa-doesnt-scale-try-reproducing-it>#</a></h2><p>So in order to fully understand the issue with the HPA not scaling the Deployment correctly during a rolling update, we recreated the failure in a kind cluster that was configured similarly to our environment. Here’s the setup:</p><h3 id=1-kind-config-with-autoscaler-delays>1. kind config with autoscaler delays<a hidden class=anchor aria-hidden=true href=#1-kind-config-with-autoscaler-delays>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># kind-config.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Cluster</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kind.x-k8s.io/v1alpha4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hpa-test</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>nodes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>control-plane</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kubeadmConfigPatches</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    kind: ClusterConfiguration
</span></span></span><span class=line><span class=cl><span class=sd>    controllerManager:
</span></span></span><span class=line><span class=cl><span class=sd>      extraArgs:
</span></span></span><span class=line><span class=cl><span class=sd>        horizontal-pod-autoscaler-initial-readiness-delay: &#34;300s&#34;
</span></span></span><span class=line><span class=cl><span class=sd>        horizontal-pod-autoscaler-sync-period: &#34;60s&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>worker</span><span class=w>
</span></span></span></code></pre></div><pre tabindex=0><code>kind create cluster --config kind-config.yaml
</code></pre><p>Those <code>extraArgs</code> on the controller manager control how the HPA behaves:</p><ul><li><p><code>horizontal-pod-autoscaler-initial-readiness-delay=300s</code> means <strong>ignore metrics from a pod for 5 minutes after it becomes Ready</strong>. This avoids scaling too early based on noisy metrics from pods that are still warming up.</p></li><li><p><code>horizontal-pod-autoscaler-sync-period=60s</code> tells the controller to <strong>evaluate metrics and scale every 60 seconds</strong>, instead of the default 15s. This makes the HPA slower to react but reduces unnecessary API churn and gives a bit more stability for workloads that spike briefly and recover.</p></li></ul><p>You might choose more aggressive settings in environments where latency matters more than stability. For us it was about confidence: letting the app settle before autoscaling decisions kick in. And making sure we’re scaling based on steady signals, not flapping probes.</p><h3 id=2-deployment-that-burns-cpu>2. Deployment that burns CPU<a hidden class=anchor aria-hidden=true href=#2-deployment-that-burns-cpu>#</a></h3><p>To simulate real-world load, we use a toy Deployment that intentionally consumes CPU. It runs a <code>busybox</code> container in a tight loop to drive usage to 100m consistently. This helps demonstrate what happens when the app is hitting resource limits and the HPA is supposed to react.</p><p>The resources section ensures each pod is requesting exactly 100m, which aligns with our HPA target later. This makes the math easier to follow.</p><p>The readiness probe here just always returns success. You might use a proper probe in real apps, but for demos we want to minimize flake.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cpu-hog</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>strategy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>RollingUpdate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollingUpdate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxSurge</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxUnavailable</span><span class=p>:</span><span class=w> </span><span class=m>0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>cpu-hog</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>cpu-hog</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hog</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>busybox</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;sh&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;while true; do :; done&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;100m&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;100m&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>readinessProbe</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>exec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;true&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>initialDelaySeconds</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>periodSeconds</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>timeoutSeconds</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span></code></pre></div><pre tabindex=0><code>kubectl apply -f cpu-hog-deployment.yaml
</code></pre><h3 id=3-attach-an-hpa>3. Attach an HPA<a hidden class=anchor aria-hidden=true href=#3-attach-an-hpa>#</a></h3><p>Next, we add a HorizontalPodAutoscaler that tracks CPU utilization. It watches the <code>cpu-hog</code> Deployment and tries to keep average usage around 50% of the requested CPU.</p><p>With our setup (each pod burns 100m and requests 100m), any pod running at full throttle will show up as 100% utilization. The HPA should see this and try to scale out to reduce average load.</p><p>We&rsquo;re keeping <code>minReplicas: 3</code> and <code>maxReplicas: 10</code>. These bounds help demonstrate scale-up and the effect of limits like <code>maxSurge</code> during rollouts.</p><p>This setup mirrors how many production services behave under bursty load, especially when request traffic ramps up faster than new pods can spin up and start reporting metrics.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>autoscaling/v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>HorizontalPodAutoscaler</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cpu-hog-hpa</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>scaleTargetRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cpu-hog</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>minReplicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>maxReplicas</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Resource</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>resource</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cpu</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>target</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Utilization</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>averageUtilization</span><span class=p>:</span><span class=w> </span><span class=m>50</span><span class=w>
</span></span></span></code></pre></div><pre tabindex=0><code>kubectl apply -f hpa.yaml
</code></pre><h3 id=4-roll-it-out-and-watch>4. Roll it out and watch<a hidden class=anchor aria-hidden=true href=#4-roll-it-out-and-watch>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl <span class=nb>set</span> image deployment/cpu-hog <span class=nv>hog</span><span class=o>=</span>busybox:1.36
</span></span><span class=line><span class=cl>kubectl get deploy cpu-hog -w
</span></span><span class=line><span class=cl>kubectl get rs -l <span class=nv>app</span><span class=o>=</span>cpu-hog -w
</span></span><span class=line><span class=cl>kubectl get pods -l <span class=nv>app</span><span class=o>=</span>cpu-hog -w
</span></span><span class=line><span class=cl>kubectl describe hpa cpu-hog-hpa
</span></span></code></pre></div><p>You&rsquo;ll see the new replicaset come up and start to scale:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>NAME                DESIRED   CURRENT   READY   AGE
</span></span><span class=line><span class=cl>cpu-hog-fb7fcf755   <span class=m>3</span>         <span class=m>3</span>         <span class=m>0</span>       27s
</span></span><span class=line><span class=cl>cpu-hog-d57478c96   <span class=m>1</span>         <span class=m>0</span>         <span class=m>0</span>       0s
</span></span><span class=line><span class=cl>cpu-hog-d57478c96   <span class=m>1</span>         <span class=m>1</span>         <span class=m>0</span>       0s
</span></span></code></pre></div><p>Looking at the pod statuses:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get pods -l <span class=nv>app</span><span class=o>=</span>cpu-hog -w
</span></span><span class=line><span class=cl>NAME                       READY   STATUS        RESTARTS   AGE
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-8n4n8   1/1     Running       <span class=m>0</span>          26s
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-dsbfk   1/1     Running       <span class=m>0</span>          24s
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-nsn6l   1/1     Running       <span class=m>0</span>          28s
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-tvh4c   1/1     Running       <span class=m>0</span>          34s
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-vrk9r   1/1     Running       <span class=m>0</span>          32s
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-zd5dq   1/1     Running       <span class=m>0</span>          30s
</span></span></code></pre></div><p>Confirm the CPU burn:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl top pods
</span></span><span class=line><span class=cl>NAME                       CPU<span class=o>(</span>cores<span class=o>)</span>   MEMORY<span class=o>(</span>bytes<span class=o>)</span>
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-8n4n8   101m         0Mi
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-dsbfk   100m         0Mi
</span></span><span class=line><span class=cl>cpu-hog-669b9bcb46-nsn6l   101m         0Mi
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>Now check how the HPA is responding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl describe hpa cpu-hog-hpa
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Metrics:                                               <span class=o>(</span> current / target <span class=o>)</span>
</span></span><span class=line><span class=cl>  resource cpu on pods  <span class=o>(</span>as a percentage of request<span class=o>)</span>:  100% <span class=o>(</span>100m<span class=o>)</span> / 50%
</span></span><span class=line><span class=cl>Min replicas:                                          <span class=m>3</span>
</span></span><span class=line><span class=cl>Max replicas:                                          <span class=m>10</span>
</span></span><span class=line><span class=cl>Deployment pods:                                       <span class=m>6</span> current / <span class=m>10</span> desired
</span></span><span class=line><span class=cl>Conditions:
</span></span><span class=line><span class=cl>  Type            Status  Reason            Message
</span></span><span class=line><span class=cl>  ----            ------  ------            -------
</span></span><span class=line><span class=cl>  AbleToScale     True    SucceededRescale  the HPA controller was able to update the target scale to <span class=m>10</span>
</span></span><span class=line><span class=cl>  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from cpu resource utilization <span class=o>(</span>percentage of request<span class=o>)</span>
</span></span><span class=line><span class=cl>  ScalingLimited  True    TooManyReplicas   the desired replica count is more than the maximum replica count
</span></span></code></pre></div><p>This is where the whole chain comes together. The HPA reads CPU metrics, calculates a new desired replica count, updates the Deployment, which updates the ReplicaSet, which spins up new Pods.</p><p>If any piece fails: readiness probe misfires, metrics aren&rsquo;t available, rollout strategy throttles new pod availability, the system “works as designed” but still fails you operationally.</p><hr><h2 id=how-the-hpa-actually-calculates-desired-replicas>How the HPA Actually Calculates Desired Replicas<a hidden class=anchor aria-hidden=true href=#how-the-hpa-actually-calculates-desired-replicas>#</a></h2><p>Here’s the formula used by the HorizontalPodAutoscaler controller:</p><pre tabindex=0><code>desiredReplicas = ceil(currentReplicas × currentMetricValue / desiredMetricValue)
</code></pre><p>If the current CPU usage is 200m and your target is 100m, the HPA will try to double your replicas:<br><code>ceil(3 × 200 / 100) = 6</code></p><p>If usage drops to 50m, you’ll likely scale down:<br><code>ceil(3 × 50 / 100) = 2</code></p><p>It skips scaling altogether if the result is close to 1.0 — by default, within 10%.</p><p>Pods that are shutting down or not Ready are ignored during calculation. So are failed pods and anything that’s missing metrics.</p><p>That means if your app takes a while to become Ready — or if your metrics backend isn’t scraping yet — the HPA might scale based on a partial view of the world.</p><p><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>Kubernetes HPA Algorithm Docs</a></p><hr><h2 id=subtle-failures-that-block-scaling>Subtle failures that block scaling<a hidden class=anchor aria-hidden=true href=#subtle-failures-that-block-scaling>#</a></h2><p>You’ve seen the happy path and an example of the unhappy path, now here are some things to look out for that might bite you.</p><h3 id=readiness-probes-that-delay-metrics>Readiness probes that delay metrics<a hidden class=anchor aria-hidden=true href=#readiness-probes-that-delay-metrics>#</a></h3><p>The HPA doesn’t count pods that aren’t Ready. If your readiness probe takes 60s, the autoscaler has to wait. Combine that with a rollout limited to one surge pod at a time, and you end up blind.</p><p>The fix is obvious, but easy to overlook. Don’t make your probe wait unless it has to. And if your cluster supports it, tune <code>--horizontal-pod-autoscaler-initial-readiness-delay</code> down from the default.</p><h3 id=cpu-requests-that-dont-reflect-reality>CPU requests that don’t reflect reality<a hidden class=anchor aria-hidden=true href=#cpu-requests-that-dont-reflect-reality>#</a></h3><p>The HPA calculates usage as a percentage of <code>requests.cpu</code>. Not actual usage. If your app regularly uses 100m but requests 300m, the HPA sees 33%. It won’t scale.</p><p>Or worse, if your request is too low, it scales too aggressively. Either way, the signal is distorted.</p><p>Set requests based on observed steady-state load. Not the worst spike you’ve ever seen. Not whatever value someone copy-pasted from a service they barely remember.</p><h3 id=pdbs-that-quietly-block-evictions>PDBs that quietly block evictions<a hidden class=anchor aria-hidden=true href=#pdbs-that-quietly-block-evictions>#</a></h3><p>Sometimes the rollout hangs. You’re stuck waiting on a pod to terminate, but it won’t. You check events and see:</p><pre tabindex=0><code>Cannot evict pod as it would violate the pod&#39;s disruption budget
</code></pre><p>That’s your PodDisruptionBudget doing its job. It’s just doing it too well. If <code>minAvailable</code> is too high, the rollout can’t proceed. And the HPA can’t scale down either.</p><p>This is easy to miss. The control plane won’t scream about it. But the rollout will stall until the PDB allows it.</p><h3 id=liveness-probes-that-kill-good-pods>Liveness probes that kill good pods<a hidden class=anchor aria-hidden=true href=#liveness-probes-that-kill-good-pods>#</a></h3><p>Under load, a pod slows down. The liveness probe fails. Kubernetes restarts it. Now the pod is unready. Metrics stop flowing. The HPA ignores it.</p><p>You get a feedback loop: load increases, restarts increase, available capacity drops, and scaling doesn’t happen.</p><p>If your app doesn’t hang, don’t use a liveness probe. Or at least configure it to tolerate spikes. Use <code>startupProbe</code> if you need one-time boot protection.</p><hr><h2 id=final-thoughts>Final thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>HPAs adjust replica counts. Deployments coordinate rollouts. ReplicaSets spin up pods. Each controller sticks to its job, and it&rsquo;s our job to understand how these things interact.</p><p>None of the controllers see the full picture. So when traffic spikes during a rollout, or metrics lag behind new pods, you end up with a system that feels stuck. Not because it’s broken, but because the coordination wasn’t built in.</p><p>Tuning this is a mix of modifying configuration options, watching where these control loops step on each other, and designing around the blind spots - especially as it relates to your application.</p><p>The more you understand how these controllers interact, the better you’ll get at shaping their behavior before an outage teaches you the hard way.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tdoot.com/tags/tech/>Tech</a></li><li><a href=https://tdoot.com/tags/kubernetes/>Kubernetes</a></li><li><a href=https://tdoot.com/tags/hpa/>Hpa</a></li></ul><nav class=paginav><a class=prev href=https://tdoot.com/writing/would-the-person-i-want-to-be-work-here/><span class=title>« Prev</span><br><span>Would the Person I Want to Be Work Here?</span>
</a><a class=next href=https://tdoot.com/writing/building-solid-kubernetes-controllers/><span class=title>Next »</span><br><span>Fundamentals for solid kubernetes controllers</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on x" href="https://x.com/intent/tweet/?text=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate&amp;url=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f&amp;hashtags=tech%2ckubernetes%2chpa"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f&amp;title=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate&amp;summary=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate&amp;source=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f&title=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on whatsapp" href="https://api.whatsapp.com/send?text=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate%20-%20https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on telegram" href="https://telegram.me/share/url?text=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate&amp;url=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How Kubernetes Deployments, ReplicaSets, and HPAs coordinate on ycombinator" href="https://news.ycombinator.com/submitlink?t=How%20Kubernetes%20Deployments%2c%20ReplicaSets%2c%20and%20HPAs%20coordinate&u=https%3a%2f%2ftdoot.com%2fwriting%2funderstanding-k8s-deployments-replicasets-hpas%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tdoot.com/>Tony De La Nuez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>